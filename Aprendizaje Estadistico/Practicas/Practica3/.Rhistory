epsilon <- rnorm(n = n, sd = sd)
# Calculamos y
y <- beta0 + beta1 * x + epsilon
z <- lm(y ~ x)
# Definimos sd
sd = 1.5
# Error (con desviación típica sd=1)
epsilon <- rnorm(n = n, sd = sd)
# Calculamos y
y <- beta0 + beta1 * x + epsilon
# Calculamos el modelo re regresión mediante `lm`
z <- lm(y ~ x)
plot(x, y, main = paste('n = ', n, '; sd = ', sd))
# Añadimos la recta de regresión
abline(z, col = "red")
library(scatterplot3d)
library(tidyverse)
library(lattice)
# Leemos el fichero 'advertising.csv' y cargamos los datos
advertising <- read_csv("data/advertising.csv")
# Cambiamos el nombre de la primera columna
colnames(advertising)[1] <- 'X'
# Leemos el fichero 'advertising.csv' y cargamos los datos
advertising <- read_csv("data/advertising.csv")
# Cambiamos el nombre de la primera columna
colnames(advertising)[1] <- 'X'
plot(advertising)
z1 <- lm(advertising$Sales ~ advertising$TV + advertising$Radio + advertising$Newspaper)
class(z1)
names(z1)
summary(z1)
coef(z1)
fitted(z1)
advertising$Sales - z1$fitted.values
residuals(z1)
summary(z)
coef(z)
gc()
gc()
rm(list = ls())
clear
library(scatterplot3d)
library(tidyverse)
library(lattice)
# Leemos el fichero 'advertising.csv' y cargamos los datos
advertising <- read_csv("data/advertising.csv")
# Cambiamos el nombre de la primera columna
colnames(advertising)[1] <- 'X'
plot(advertising)
z1 <- lm(advertising$Sales ~ advertising$TV + advertising$Radio + advertising$Newspaper)
class(z1)
names(z1)
summary(z1)
coef(z1)
# Definimos y:
y <- data.matrix(advertising[5])
# Definimos X:
X <- cbind(rep(1, length(y)), data.matrix(advertising[2:4]))
# Ahora calculamos beta_hat
beta_hat <- solve((t(X) %*% X)) %*% t(X) %*% y
cat("Calculados mediante la formula:\n")
print(beta_hat)
cat("\nCalculados mediante la función `lm`:\n")
print(coef(z1))
fitted(z1)
advertising$Sales - z1$fitted.values
residuals(z1)
# Calculamos RSS
RSS <- sum(as.vector(residuals(z1)) * as.vector(residuals(z1)))
# Definimos n y p
n <- length(y)
p <- 3
# Calcualmos RSE
RSE <- sqrt(RSS / (n - p - 1))
print(paste("RSE:", RSE, sep = " "))
summary(z1)
confint(z1, level = 0.9)
summary(z1)
z2 <- lm(Sales ~ TV + Radio, data = advertising)
# Para plotear en un grid las diferentes gráficas
par(mfrow=c(2,2))
# Ploteamos
plot(z2)
summary(z2)
newdata = data.frame(TV = 100, Radio = 20)
predict(z2, newdata)
predict(z2, newdata, interval = "confidence")
predict(z2, newdata, interval = "predict")
n <- 100
# x_i: n puntos aleatorios en el itervalo [min,max]
x <- runif(n, min = 0, max = 5)
# Parámetro beta0 del modelo
beta0 <- 2
# Parámetro beta1 del modelo
beta1 <- 5
# Error (con desviación típica sd=1)
epsilon <- rnorm(n, sd = 1)
# Calculamos y
y <- beta0 + beta1 * x + epsilon
plot(x, y, main = paste('n =  ', n, '; sd = 1'))
# Para plotear en un grid las diferentes gráficas
par(mfrow=c(2,2))
for (i in 1:4) {
# Definimos sd
sd = 0.5 * i^2
# Error (con desviación típica sd=1)
epsilon <- rnorm(n=n, sd=sd)
# Calculamos y
y <- beta0 + beta1 * x + epsilon
# Ploteamos
plot(x, y, main = paste('n = ', n, '; sd = ', sd))
}
# Definimos sd
sd = 1.5
# Error (con desviación típica sd=1)
epsilon <- rnorm(n = n, sd = sd)
# Calculamos y
y <- beta0 + beta1 * x + epsilon
# Calculamos el modelo re regresión mediante `lm`
z <- lm(y ~ x)
plot(x, y, main = paste('n = ', n, '; sd = ', sd))
# Añadimos la recta de regresión
abline(z, col = "red")
summary(z)
coef(z)
# Función a modelar
f <- function(x, beta0, beta1) {
return(beta0 + beta1 * x)
}
# RSE (Residual Standard Error)
rse <- function(reales, prediccion) {
n <- length(reales)
rss <- sum((prediccion - reales) ^ 2)
rse <- sqrt((rss / (n - 2)))
return(rse)
}
# Función de coste
coste <- function(x, y, beta0, beta1) {
prediccion <- f(x, beta0, beta1)
rse <- rse(y, prediccion)
return(rse)
}
# Función del descenso de gradiente
desc_grad <- function(x, y, t, threshold,  max_iter) {
# Inicializamos los coeficientes a 0
beta0 <- 0
beta1 <- 0
converged = FALSE
iterations = 0
while (converged == FALSE) {
# Obtenemos la prediccion
prediccion <- f(x, beta0, beta1)
# Calculamos el gradiente de f(x)
grad_beta0 = sum(y - prediccion)
grad_beta1 = sum(x * (y - prediccion))
# Actualizamos los valores de los parámetros beta0 y beta1
beta0 <- beta0 + t * grad_beta0
beta1 <- beta1 + t * grad_beta1
# Calculamos el rse cometido con la función de coste
coste <- coste(x, y, beta0, beta1)
# Obtenemos el criterio de parada
stopping_criterion = grad_beta0^2 + grad_beta1^2
# Comprobamos si se cumple el criterio de parada
if (stopping_criterion <= threshold) {
converged = T
print(
paste(
"Optimal beta0: ",
beta0,
"; Optimal beta1: ",
beta1,
"; RSE: ",
coste,
"; Converged in ",
iterations,
" iterations"
)
)
}
# Comprobamos si se han ejecutado el número máximo de iteraciones
iterations = iterations + 1
if (iterations > max_iter) {
converged = TRUE
print(
paste(
"Optimal beta0: ",
beta0,
"; Optimal beta1: ",
beta1,
"; RSE: ",
coste,
"; Not Converged"
)
)
}
}
return(list(beta0, beta1, coste))
}
result <- desc_grad(x, y, t = 0.001, threshold = 0.001, max_iter = 1000)
par(mfrow = c(1, 2))
plot(x, y, main = "Método del gradiente")
lines(c(-10:10), f(c(-10:10), result[[1]], result[[2]]), col = "red", lwd = 1)
plot(x, y, main = "Función `lm`")
abline(z, col = "red")
print(paste("Descenso del gradiente:"))
print(paste("beta0: ", result[[1]], "; beta1: ", result[[2]], "; RSE: ", result[[3]]))
cat("\n")
print(paste("Función `lm`:"))
print(paste("beta0: ", z$coefficients[[1]], "; beta1: ", z$coefficients[[2]], "; RSE: ", sigma(z)))
print(paste("Descenso del gradiente:"))
print(paste("beta0: ", result[[1]], "; beta1: ", result[[2]], "; RSE: ", result[[3]]))
cat("\n")
print(paste("Función `lm`:"))
print(paste("beta0: ", z$coefficients[[1]], "; beta1: ", z$coefficients[[2]], "; RSE: ", sigma(z)))
library(scatterplot3d)
library(tidyverse)
library(lattice)
# Leemos el fichero 'advertising.csv' y cargamos los datos
advertising <- read_csv("data/advertising.csv")
# Cambiamos el nombre de la primera columna
colnames(advertising)[1] <- 'X'
plot(advertising)
z1 <- lm(advertising$Sales ~ advertising$TV + advertising$Radio + advertising$Newspaper)
class(z1)
names(z1)
summary(z1)
coef(z1)
# Definimos y:
y <- data.matrix(advertising[5])
# Definimos X:
X <- cbind(rep(1, length(y)), data.matrix(advertising[2:4]))
# Ahora calculamos beta_hat
beta_hat <- solve((t(X) %*% X)) %*% t(X) %*% y
cat("Calculados mediante la formula:\n")
print(beta_hat)
cat("\nCalculados mediante la función `lm`:\n")
print(coef(z1))
fitted(z1)
advertising$Sales - z1$fitted.values
residuals(z1)
# Calculamos RSS
RSS <- sum(as.vector(residuals(z1)) * as.vector(residuals(z1)))
# Definimos n y p
n <- length(y)
p <- 3
# Calcualmos RSE
RSE <- sqrt(RSS / (n - p - 1))
print(paste("RSE:", RSE, sep = " "))
summary(z1)
confint(z1, level = 0.9)
summary(z1)
z2 <- lm(Sales ~ TV + Radio, data = advertising)
# Para plotear en un grid las diferentes gráficas
par(mfrow=c(2,2))
# Ploteamos
plot(z2)
summary(z2)
newdata = data.frame(TV = 100, Radio = 20)
predict(z2, newdata)
predict(z2, newdata, interval = "confidence")
predict(z2, newdata, interval = "predict")
n <- 100
# x_i: n puntos aleatorios en el itervalo [min,max]
x <- runif(n, min = 0, max = 5)
# Parámetro beta0 del modelo
beta0 <- 2
# Parámetro beta1 del modelo
beta1 <- 5
# Error (con desviación típica sd=1)
epsilon <- rnorm(n, sd = 1)
# Calculamos y
y <- beta0 + beta1 * x + epsilon
plot(x, y, main = paste('n =  ', n, '; sd = 1'))
# Para plotear en un grid las diferentes gráficas
par(mfrow=c(2,2))
for (i in 1:4) {
# Definimos sd
sd = 0.5 * i^2
# Error (con desviación típica sd=1)
epsilon <- rnorm(n=n, sd=sd)
# Calculamos y
y <- beta0 + beta1 * x + epsilon
# Ploteamos
plot(x, y, main = paste('n = ', n, '; sd = ', sd))
}
# Definimos sd
sd = 1.5
# Error (con desviación típica sd=1)
epsilon <- rnorm(n = n, sd = sd)
# Calculamos y
y <- beta0 + beta1 * x + epsilon
# Calculamos el modelo re regresión mediante `lm`
z <- lm(y ~ x)
plot(x, y, main = paste('n = ', n, '; sd = ', sd))
# Añadimos la recta de regresión
abline(z, col = "red")
summary(z)
coef(z)
# Función a modelar
f <- function(x, beta0, beta1) {
return(beta0 + beta1 * x)
}
# RSE (Residual Standard Error)
rse <- function(reales, prediccion) {
n <- length(reales)
rss <- sum((prediccion - reales) ^ 2)
rse <- sqrt((rss / (n - 2)))
return(rse)
}
# Función de coste
coste <- function(x, y, beta0, beta1) {
prediccion <- f(x, beta0, beta1)
rse <- rse(y, prediccion)
return(rse)
}
# Función del descenso de gradiente
desc_grad <- function(x, y, t, threshold,  max_iter) {
# Inicializamos los coeficientes a 0
beta0 <- 0
beta1 <- 0
converged = FALSE
iterations = 0
while (converged == FALSE) {
# Obtenemos la prediccion
prediccion <- f(x, beta0, beta1)
# Calculamos el gradiente de f(x)
grad_beta0 = sum(y - prediccion)
grad_beta1 = sum(x * (y - prediccion))
# Actualizamos los valores de los parámetros beta0 y beta1
beta0 <- beta0 + t * grad_beta0
beta1 <- beta1 + t * grad_beta1
# Calculamos el rse cometido con la función de coste
coste <- coste(x, y, beta0, beta1)
# Obtenemos el criterio de parada
stopping_criterion = grad_beta0^2 + grad_beta1^2
# Comprobamos si se cumple el criterio de parada
if (stopping_criterion <= threshold) {
converged = T
print(
paste(
"Optimal beta0: ",
beta0,
"; Optimal beta1: ",
beta1,
"; RSE: ",
coste,
"; Converged in ",
iterations,
" iterations"
)
)
}
# Comprobamos si se han ejecutado el número máximo de iteraciones
iterations = iterations + 1
if (iterations > max_iter) {
converged = TRUE
print(
paste(
"Optimal beta0: ",
beta0,
"; Optimal beta1: ",
beta1,
"; RSE: ",
coste,
"; Not Converged"
)
)
}
}
return(list(beta0, beta1, coste))
}
result <- desc_grad(x, y, t = 0.001, threshold = 0.001, max_iter = 1000)
par(mfrow = c(1, 2))
plot(x, y, main = "Método del gradiente")
lines(c(-10:10), f(c(-10:10), result[[1]], result[[2]]), col = "red", lwd = 1)
plot(x, y, main = "Función `lm`")
abline(z, col = "red")
print(paste("Descenso del gradiente:"))
print(paste("beta0: ", result[[1]], "; beta1: ", result[[2]], "; RSE: ", result[[3]]))
cat("\n")
print(paste("Función `lm`:"))
print(paste("beta0: ", z$coefficients[[1]], "; beta1: ", z$coefficients[[2]], "; RSE: ", sigma(z)))
# Función del descenso de gradiente
desc_grad_stoch <- function(x, y, t, threshold,  max_iter) {
# Inicializamos los coeficientes a 0
beta0 <- 0
beta1 <- 0
converged = FALSE
iterations = 0
while (converged == FALSE) {
# Tamaño de la muestra
n <- length(x)
for (i in 1:n){
# Obtenemos la prediccion
prediccion <- f(x[i], beta0, beta1)
# Actualizamos los valores de los parámetros beta0 y beta1
beta0 <- beta0 + t * (y[i] - prediccion)
beta1 <- beta1 + t * (x[i] * (y[i] - prediccion))
}
# Calculamos el rse cometido con la función de coste
coste <- coste(x, y, beta0, beta1)
# Obtenemos el criterio de parada:
prediccion <- f(x, beta0, beta1)
grad_beta0 = sum(y - prediccion)
grad_beta1 = sum(x * (y - prediccion))
stopping_criterion = grad_beta0^2 + grad_beta1^2
# Comprobamos si se cumple el criterio de parada
if (stopping_criterion <= threshold) {
converged = T
print(
paste(
"Optimal beta0: ",
beta0,
"; Optimal beta1: ",
beta1,
"; RSE: ",
coste,
"; Converged in ",
iterations,
" iterations"
)
)
}
# Comprobamos si se han ejecutado el número máximo de iteraciones
iterations = iterations + 1
if (iterations > max_iter) {
converged = TRUE
print(
paste(
"Optimal beta0: ",
beta0,
"; Optimal beta1: ",
beta1,
"; RSE: ",
coste,
"; Not Converged"
)
)
}
}
return(list(beta0, beta1, coste))
}
result <- desc_grad_stoch(x, y, t = 0.001, threshold = 0.001, max_iter = 1000)
# Función del descenso de gradiente
desc_grad_stoch <- function(x, y, t, threshold,  max_iter) {
# Inicializamos los coeficientes a 0
beta0 <- 0
beta1 <- 0
converged = FALSE
iterations = 0
while (converged == FALSE) {
# Tamaño de la muestra
n <- length(x)
for (i in 1:n){
# Obtenemos la prediccion
prediccion <- f(x[i], beta0, beta1)
# Actualizamos los valores de los parámetros beta0 y beta1
beta0 <- beta0 + t * (y[i] - prediccion)
beta1 <- beta1 + t * (x[i] * (y[i] - prediccion))
}
# Calculamos el rse cometido con la función de coste
coste <- coste(x, y, beta0, beta1)
# Obtenemos el criterio de parada:
prediccion <- f(x, beta0, beta1)
stopping_criterion = sum(y - prediccion)^2 + sum(x * (y - prediccion))^2
# Comprobamos si se cumple el criterio de parada
if (stopping_criterion <= threshold) {
converged = T
print(
paste(
"Optimal beta0: ",
beta0,
"; Optimal beta1: ",
beta1,
"; RSE: ",
coste,
"; Converged in ",
iterations,
" iterations"
)
)
}
# Comprobamos si se han ejecutado el número máximo de iteraciones
iterations = iterations + 1
if (iterations > max_iter) {
converged = TRUE
print(
paste(
"Optimal beta0: ",
beta0,
"; Optimal beta1: ",
beta1,
"; RSE: ",
coste,
"; Not Converged"
)
)
}
}
return(list(beta0, beta1, coste))
}
result <- desc_grad_stoch(x, y, t = 0.001, threshold = 0.001, max_iter = 3000)
par(mfrow = c(1, 2))
plot(x, y, main = "Método del gradiente")
lines(c(-10:10), f(c(-10:10), result[[1]], result[[2]]), col = "red", lwd = 1)
plot(x, y, main = "Función `lm`")
abline(z, col = "red")
print(paste("Descenso del gradiente estocástico:"))
print(paste("beta0: ", result[[1]], "; beta1: ", result[[2]], "; RSE: ", result[[3]]))
cat("\n")
print(paste("Función `lm`:"))
print(paste("beta0: ", z$coefficients[[1]], "; beta1: ", z$coefficients[[2]], "; RSE: ", sigma(z)))
