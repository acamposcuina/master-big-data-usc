---
title: 'Práctica 3: Modelos de Regresión Lineal con R'
author: "Andrés Campos Cuiña"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output:
  pdf_document:
    number_sections: yes
    toc: no
  html_document:
    df_print: paged
    toc: no
  word_document:
    toc: yes
header-includes:
- \usepackage[utf8]{inputenc}
- \usepackage[spanish]{babel}
---

\pagenumbering{gobble} 

<style>
  body {
    text-align : justify
  }
</style>

\cleardoublepage
\tableofcontents
\cleardoublepage

\pagenumbering{arabic} 

# Carga de las librerías necesarias

Primero cargamos las librerías necesarias:

```{r, message=FALSE}
library(scatterplot3d)
library(tidyverse)
library(lattice)
```

# Ajuste de un modelo de regresión lineal con R

En primer lugar tenemos que cargar los datos a partir del fichero `advertising.csv` y después depuramos estos datos eliminando la primera columna del `data.frame` obtenido, ya que esta columna sólo contiene el número de la fila en la que nos encontramos, lo que no nos será necesario.

```{r, message=FALSE}
# Leemos el fichero 'advertising.csv' y cargamos los datos
advertising <- read_csv("data/advertising.csv")

# Cambiamos el nombre de la primera columna
colnames(advertising)[1] <- 'X'
```

Una vez hemos leído los datos y los hemos almacenado en un objeto del tipo `data.frame` podemos representar como se relacionan las ventas y la inversión en publicidad en distintos medios mediante el uso de la función `plot()`:

```{r}
plot(advertising)
```

Como podemos ver en la gráfica superior, algunas de las variables si que muestran de manera visual algún tipo de correlación con la variable de ventas (`Sales`). En especial, muestra bastante correlación con la variable de ventas la variable de `TV`. También muestra bastante correlación con la variable de ventas la variable `Radio`, por el contrario no se aprecia tanta correlación con la variable `Newspaper`. 

Realizamos el ajuste lineal en R:

```{r}
z1 <- lm(advertising$Sales ~ advertising$TV + advertising$Radio + advertising$Newspaper)
```

Esta función devuelve un objeto `lm` con varias componentes.

```{r}
class(z1)
```

Las componentes con las que cuenta el objeto `lm` devuelto son las siguientes:

```{r}
names(z1)
```

Se puede resumir los resultados del ajuste mediante la función `summary`:

```{r}
summary(z1)
```

Como se puede ver en el resumen obtenido, y a través de los explicado en clase como el `p-value` obtenido para la variable `Newspaper` es bastante alto (del 0,86) podríamos decir que no hay correlación entra las ventas y el valor de la variable `Newspaper`.

## Estimación de los parámetros del modelo

Obtenemos los coeficientes estimados del modelo mediante la función `coef`:

```{r}
coef(z1)
```

Comprobamos que los coeficientes estimados del modelo se obtienen como: 

$\hat{\beta } = (X^{t}X)^{-1}X^{t}y$

```{r}
# Definimos y:
y <- data.matrix(advertising[5])

# Definimos X:
X <- cbind(rep(1, length(y)), data.matrix(advertising[2:4]))

# Ahora calculamos beta_hat
beta_hat <- solve((t(X) %*% X)) %*% t(X) %*% y

```

De esta manera se puede ver como los coeficientes ajustado por el método de mínimos cuadrados coinciden con los que acabamos de calcular:

```{r}
cat("Calculados mediante la formula:\n")
print(beta_hat)

cat("\nCalculados mediante la función `lm`:\n")
print(coef(z1))
```

Obtenemos los valores ajustados por el modelo de regresión lineal, $\hat{y_i}$, mediante la función `fitted`:

```{r}
fitted(z1)
```

Siendo los residuos del modelo, $\hat{\epsilon_i}$, los siguientes:

```{r}
advertising$Sales - z1$fitted.values
```

Equivalentemente:

```{r}
residuals(z1)
```

Ahora calcularemos el RSE a partir de los residuos del modelo para comprobar que se obtiene el mismo valor que si usasemos la función `summary`. Para esto haremos uso de la siguiente fórmula:

$RSE = \sqrt{\frac{RSS}{n-p-1}}$

Programamos este cálculo:

```{r}
# Calculamos RSS
RSS <- sum(as.vector(residuals(z1)) * as.vector(residuals(z1)))

# Definimos n y p
n <- length(y)
p <- 3

# Calcualmos RSE
RSE <- sqrt(RSS / (n - p - 1))

print(paste("RSE:", RSE, sep = " "))

```

Mediante la función `summary` podemos ver como el valor del RSE es de 1,686 en ambos casos:

```{r}
summary(z1)
```

Podemos obtener intervalos de confianza para los coeficientes del modelo con la función `confint` :

```{r}
confint(z1, level = 0.9)

```

## Contrastes sobre los parámetros del modelo:

Analizamos los resultados de los contrastes sobre los parámetros del modelo que devuelve la función `summary`:

```{r}
summary(z1)

```

Tal como se explica en el enunciado, podemos ver como (mediante la función `summary`) las variables TV y Radio sí sin significativas mientras que la variable Newspaper no lo es. Por lo tanto ahora ajustamos un modelo que utilice únicamente las variables TV y Radio para predecir Sales:

```{r}
z2 <- lm(Sales ~ TV + Radio, data = advertising)

# Para plotear en un grid las diferentes gráficas
par(mfrow=c(2,2))

# Ploteamos
plot(z2)

```
Con este nuevo modelo podemos observar como la proporción de la variabilidad de Sales mediante las otras dos variables predictores es ligeramente mayor que en el modelo anterior en el que se usaba también la variable Newspaper:

```{r}
summary(z2)

```

Por lo tanto, podemos concluir que este segundo modelo es más razonable.

## Predicción

Para predecir nuevos valores para la variable Sales a partir de los valores de TV y de Radio escribimos:

```{r}
newdata = data.frame(TV = 100, Radio = 20)
predict(z2, newdata)

```

Como podemos ver se esperarían unas ventas de 11,25647 miles de dolares.
También podemos ver los intervalos de confianza y de predicción para estos valores para las variables TV y Radio, como se puede ver el intervalo de predicción es mayor:

```{r}
predict(z2, newdata, interval = "confidence")

```


```{r}
predict(z2, newdata, interval = "predict")

```


# Ajuste de los parámetros de un modelo de regresión lineal mediante métodos de optimización

## Método de descenso de gradiente

Para simplificar el proceso consideraremos ahora un modelo de regresión lineal simple, como el siguiente:

$$Y = \beta_0 + \beta_1X_1 + \epsilon$$

En primer lugar, vamos a simular una muestra con $n = 100$ observaciones, $(x_i, y_i)$, de un modelo de regresión lineal simple con parámetros $\beta_0$ y $\beta_1$. Para ello, generamos en primer lugar los valores $x_i$ a partir de una distribución uniforme. A continuación generamos los errores del modelo, $\epsilon_i$, a partir de una distribución normal de media 0 y varianza $\sigma^2$. Los valores $y_i$ se calcularán entonces como:

$$y_i = \beta_0 + \beta_1x_i + \epsilon_i$$

En R, por lo tanto, empezamos generando nuestra muestra de 100 observaciones:

```{r}
n <- 100
  
# x_i: n puntos aleatorios en el itervalo [min,max]
x <- runif(n, min = 0, max = 5) 

# Parámetro beta0 del modelo
beta0 <- 2 

 # Parámetro beta1 del modelo
beta1 <- 5

# Error (con desviación típica sd=1)
epsilon <- rnorm(n, sd = 1) 

# Calculamos y
y <- beta0 + beta1 * x + epsilon

```

En primer lugar haremos un diagrama de dispersión de la muestra de entrenamiento generada:

```{r}
plot(x, y, main = paste('n =  ', n, '; sd = 1'))

```

Podemos representar este diagrama para diferentes valores del parámetro `sd`:

```{r}
# Para plotear en un grid las diferentes gráficas
par(mfrow=c(2,2))

for (i in 1:4) {
  # Definimos sd
  sd = 0.5 * i^2
  
  # Error (con desviación típica sd=1)
  epsilon <- rnorm(n=n, sd=sd)
  
  # Calculamos y
  y <- beta0 + beta1 * x + epsilon
  
  # Ploteamos
  plot(x, y, main = paste('n = ', n, '; sd = ', sd))
}


```

Como se puede observar, cuanto mayor es el valor de $\sigma^2$, `sd`, mayor es el valor de cada uno de los $\epsilon_i$ y por lo tanto también hay menos correlación entre la variable `y` y la variable `x`.

Ahora calcularemos el valor de los parámetros estimados ($\beta_0$ y $\beta_1$) mediante la función `lm`:

```{r}
# Definimos sd
sd = 1.5

# Error (con desviación típica sd=1)
epsilon <- rnorm(n = n, sd = sd)

# Calculamos y
y <- beta0 + beta1 * x + epsilon

# Calculamos el modelo re regresión mediante `lm`
z <- lm(y ~ x)

```

Visualizamos el resultado obtenido mediante la función `lm`:

```{r}
plot(x, y, main = paste('n = ', n, '; sd = ', sd))

# Añadimos la recta de regresión
abline(z, col = "red")

```
Resumimos los resultados del ajuste mediante la función `summary`:

```{r}
summary(z)
```

Obtenemos los parámetros estimados por el modelo mediante la función `coef`:

```{r}
coef(z)
```

Ahora aproximaremos estos parámetros mediante la el método del descenso de gradiente. Para ello hay que minimizar la siguiente función:

$$J(\beta_0, \beta_1) = \frac{1}{2} \sum_{i=1}^{n}(y_i - \beta_0 + \beta_1x_i)^2$$

Por lo tanto, programamos el método del descenso de gradiente para minimizar esta función. Lo primero será definir la función `f` que vamos a minimizar, la función `mse` que usaremos para calcular el error cuadrático medio y la función `coste` que usaremos para calcular el MSE para nuestro valor en cada iteración para $\beta_0$ y $\beta_1$.

```{r}
# Función a modelar
f <- function(x, beta0, beta1) {
  return(beta0 + beta1 * x)
}

# RSE (Residual Standard Error)
rse <- function(reales, prediccion) {
  n <- length(reales)
  rss <- sum((prediccion - reales) ^ 2)
  rse <- sqrt((rss / (n - 2)))
  return(rse)
}

# Función de coste
coste <- function(x, y, beta0, beta1) {
  prediccion <- f(x, beta0, beta1)
  rse <- rse(y, prediccion)
  return(rse)
}

```

Ahora implementaremos el método del descenso de gradiente:

```{r}
# Función del descenso de gradiente
desc_grad <- function(x, y, t, threshold,  max_iter) {
  # Inicializamos los coeficientes a 0
  beta0 <- 0
  beta1 <- 0
  
  converged = FALSE
  iterations = 0
  
  while (converged == FALSE) {
    # Obtenemos la prediccion
    prediccion <- f(x, beta0, beta1)
    
    # Calculamos el gradiente de f(x)
    grad_beta0 = sum(y - prediccion)
    grad_beta1 = sum(x * (y - prediccion))
    
    # Actualizamos los valores de los parámetros beta0 y beta1
    beta0 <- beta0 + t * grad_beta0
    beta1 <- beta1 + t * grad_beta1
    
    # Calculamos el rse cometido con la función de coste
    coste <- coste(x, y, beta0, beta1)
    
    # Obtenemos el criterio de parada
    stopping_criterion = grad_beta0^2 + grad_beta1^2
    
    # Comprobamos si se cumple el criterio de parada
    if (stopping_criterion <= threshold) {
      converged = T
      print(
        paste(
          "Optimal beta0: ",
          beta0,
          "; Optimal beta1: ",
          beta1,
          "; RSE: ",
          coste,
          "; Converged in ",
          iterations,
          " iterations"
        )
      )
    }
    
    # Comprobamos si se han ejecutado el número máximo de iteraciones
    iterations = iterations + 1
    if (iterations > max_iter) {
      converged = TRUE
      print(
        paste(
          "Optimal beta0: ",
          beta0,
          "; Optimal beta1: ",
          beta1,
          "; RSE: ",
          coste,
          "; Not Converged"
        )
      )
    }
  }
  return(list(beta0, beta1, coste))
}

```

Ejecutamos nuestra implementación del descenso de gradiente, con un paso de 0,001 y un número de iteraciones máximas igual a 1000. Consideraremos que un valor para el criterio de parada, para el que usaremos $||\nabla f(x)||^2$.

```{r}
result <- desc_grad(x, y, t = 0.001, threshold = 0.001, max_iter = 1000)

```

Como se puede ver, nuestra implementación del descenso del gradiente aproxima, en 157 iteraciones, unos valores para $\beta_0$ igual a 2,0095 y un valor para $\beta_1$ igual a 4,9592. Además nos devuelve un valor para el RSE igual a 1,3891.

Compararemos mediante dos gráficas la recta de regresión obtenida por nuestro modelo y la obtenida mediante el uso de la función `lm`:

```{r}
par(mfrow = c(1, 2))

plot(x, y, main = "Método del gradiente")
lines(c(-10:10), f(c(-10:10), result[[1]], result[[2]]), col = "red", lwd = 1)

plot(x, y, main = "Función `lm`")
abline(z, col = "red")

```
Como se puede observar la recta obtenida por ambos métodos es prácticamente la misma. Si comparamos los valores obtenidos por cada método obtenemos:

```{r}
print(paste("Descenso del gradiente:"))
print(paste("beta0: ", result[[1]], "; beta1: ", result[[2]], "; RSE: ", result[[3]]))

cat("\n")

print(paste("Función `lm`:"))
print(paste("beta0: ", z$coefficients[[1]], "; beta1: ", z$coefficients[[2]], "; RSE: ", sigma(z)))

```


## Método de descenso de gradiente estocástico (Stochastic Gradient Descent)

Ahora implementaremos el método del descenso del gradiente estocástico:

```{r}
# Función del descenso de gradiente
desc_grad_stoch <- function(x, y, t, threshold,  max_iter) {
  # Inicializamos los coeficientes a 0
  beta0 <- 0
  beta1 <- 0
  
  converged = FALSE
  iterations = 0
  
  while (converged == FALSE) {
    # Tamaño de la muestra
    n <- length(x)
    
    for (i in 1:n){
      # Obtenemos la prediccion
      prediccion <- f(x[i], beta0, beta1)
      
      # Actualizamos los valores de los parámetros beta0 y beta1
      beta0 <- beta0 + t * (y[i] - prediccion)
      beta1 <- beta1 + t * (x[i] * (y[i] - prediccion))
    }
    
    # Calculamos el rse cometido con la función de coste
    coste <- coste(x, y, beta0, beta1)
    
    # Obtenemos el criterio de parada:
    prediccion <- f(x, beta0, beta1)
    stopping_criterion = sum(y - prediccion)^2 + sum(x * (y - prediccion))^2
    
    # Comprobamos si se cumple el criterio de parada
    if (stopping_criterion <= threshold) {
      converged = T
      print(
        paste(
          "Optimal beta0: ",
          beta0,
          "; Optimal beta1: ",
          beta1,
          "; RSE: ",
          coste,
          "; Converged in ",
          iterations,
          " iterations"
        )
      )
    }
    
    # Comprobamos si se han ejecutado el número máximo de iteraciones
    iterations = iterations + 1
    if (iterations > max_iter) {
      converged = TRUE
      print(
        paste(
          "Optimal beta0: ",
          beta0,
          "; Optimal beta1: ",
          beta1,
          "; RSE: ",
          coste,
          "; Not Converged"
        )
      )
    }
  }
  return(list(beta0, beta1, coste))
}

```


```{r}
result <- desc_grad_stoch(x, y, t = 0.001, threshold = 0.001, max_iter = 3000)
```

```{r}
par(mfrow = c(1, 2))

plot(x, y, main = "Método del gradiente estocástico")
lines(c(-10:10), f(c(-10:10), result[[1]], result[[2]]), col = "red", lwd = 1)

plot(x, y, main = "Función `lm`")
abline(z, col = "red")
```

```{r}
print(paste("Descenso del gradiente estocástico:"))
print(paste("beta0: ", result[[1]], "; beta1: ", result[[2]], "; RSE: ", result[[3]]))

cat("\n")

print(paste("Función `lm`:"))
print(paste("beta0: ", z$coefficients[[1]], "; beta1: ", z$coefficients[[2]], "; RSE: ", sigma(z)))
```

# Problemas en el ajuste del método de regresión lineal

## Efecto de la multicolinealidad en la estimación del modelo de regresión lineal por el método de mínimos cuadrados

Sigue pero me da perezote...







